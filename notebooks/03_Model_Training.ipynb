{"cells":[{"cell_type":"markdown","metadata":{"id":"YwgZb3MpUHWe"},"source":["# Model Training & Hyperparameter Tuning\n","\n","This notebook focuses on training different models for sentiment classification:\n","\n","1. Baseline Models:\n","   - Logistic Regression\n","   - Support Vector Machine (SVM)\n","   - Random Forest\n","\n","2. Deep Learning Models:\n","   - BiLSTM\n","   - BERT fine-tuning\n","\n","3. Hyperparameter Tuning:\n","   - Grid Search for baseline models\n","   - Optuna for deep learning models\n","\n","We will train each model with different feature representations and save the best models."]},{"cell_type":"markdown","metadata":{"id":"q0_88KLSUHWf"},"source":["## Setup and Imports"]},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hQOCgVy_o_kQ","executionInfo":{"status":"ok","timestamp":1744519082958,"user_tz":-480,"elapsed":14158,"user":{"displayName":"Wong Matthew","userId":"12913150185447736015"}},"outputId":"bacc3462-4fd4-4713-c2d4-cf1865552d29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.1)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Installing collected packages: colorlog, alembic, optuna\n","Successfully installed alembic-1.15.2 colorlog-6.9.0 optuna-4.2.1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3GDBuZMeUHWf"},"source":["# Core libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pickle\n","import os\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Machine learning libraries\n","from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, roc_auc_score\n","import scipy.sparse as sp\n","\n","# Deep learning libraries\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from torch.optim import AdamW\n","\n","# Hyperparameter optimization\n","import optuna\n","\n","# Visualization settings\n","plt.style.use('ggplot')\n","sns.set(style='whitegrid')\n","%matplotlib inline"],"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PjKTFwD0UHWg"},"source":["## Load Features and Prepare Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"618O35qMUHWg","executionInfo":{"status":"ok","timestamp":1744519155714,"user_tz":-480,"elapsed":30083,"user":{"displayName":"Wong Matthew","userId":"12913150185447736015"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"21958d07-da54-4993-9b0f-56636c7e23c9"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","data_dir = '/content/drive/MyDrive/Colab Notebooks/is5126/final-project/data'\n","features_dir = '/content/drive/MyDrive/Colab Notebooks/is5126/final-project/data/features'\n","models_dir = '/content/drive/MyDrive/Colab Notebooks/is5126/final-project/models'\n","results_dir = '/content/drive/MyDrive/Colab Notebooks/is5126/final-project/results'\n","\n","# Load pre-split datasets\n","train_df = pd.read_csv(os.path.join(data_dir, 'twitter_training_clean.csv'))\n","val_df = pd.read_csv(os.path.join(data_dir, 'twitter_validation_clean.csv'))\n","test_df = pd.read_csv(os.path.join(data_dir, 'twitter_testing_clean.csv'))\n","\n","# Extract labels\n","y_train = train_df['sentiment'].values\n","y_val = val_df['sentiment'].values\n","y_test = test_df['sentiment'].values\n","\n","# Define a function to load features\n","def load_features(feature_type, feature_dir):\n","  if feature_type in ['bow', 'tfidf']:\n","    return (\n","      sp.load_npz(os.path.join(feature_dir, f'{feature_type}_features_train.npz')),\n","      sp.load_npz(os.path.join(feature_dir, f'{feature_type}_features_val.npz')),\n","      sp.load_npz(os.path.join(feature_dir, f'{feature_type}_features_test.npz'))\n","    )\n","  else:\n","    return (\n","      np.load(os.path.join(feature_dir, f'{feature_type}_features_train.npy')),\n","      np.load(os.path.join(feature_dir, f'{feature_type}_features_val.npy')),\n","      np.load(os.path.join(feature_dir, f'{feature_type}_features_test.npy'))\n","    )\n","\n","# Load features using the function\n","X_bow_train, X_bow_val, X_bow_test = load_features('bow', features_dir)\n","X_tfidf_train, X_tfidf_val, X_tfidf_test = load_features('tfidf', features_dir)\n","X_word2vec_train, X_word2vec_val, X_word2vec_test = load_features('word2vec', features_dir)\n","X_glove_train, X_glove_val, X_glove_test = load_features('glove', features_dir)\n","X_bert_train, X_bert_val, X_bert_test = load_features('bert', features_dir)\n","print(\"Features loaded successfully.\")\n","\n","\n","# Load the label encoder first (add this with your other imports)\n","with open(os.path.join(models_dir, 'label_encoder.pkl'), 'rb') as f:\n","    label_encoder = pickle.load(f)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Features loaded successfully.\n"]}]},{"cell_type":"markdown","metadata":{"id":"n23BFpv6UHWh"},"source":["## Baseline Models\n","For baseline models, we focus on comparing all five features to understand the difference. We also make use of grid_search to play around different set of hyperparameters to compare."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uelvMsSHUHWh"},"source":["# Train and evaluate a model\n","def train_evaluate_model(model, X_train, X_val, y_train, y_val, model_name, feature_name):\n","    # Train model\n","    model.fit(X_train, y_train)\n","\n","    # Make predictions\n","    y_pred = model.predict(X_val)\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(y_val, y_pred)\n","    f1 = f1_score(y_val, y_pred, average='weighted')\n","\n","    # Print results\n","    print(f\"{model_name} with {feature_name} - Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n","\n","    return {\n","        'model': model,\n","        'accuracy': accuracy,\n","        'f1_score': f1,\n","        'model_name': model_name,\n","        'feature_name': feature_name\n","    }\n","\n","# Function to run GridSearchCV\n","def grid_search_model(model, param_grid, X_train, y_train, model_name):\n","    # Set up GridSearchCV\n","    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","    grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='f1_weighted', n_jobs=-1, verbose=1)\n","\n","    # Train model with grid search\n","    grid_search.fit(X_train, y_train)\n","\n","    # Print results\n","    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n","    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n","\n","    return grid_search.best_estimator_"],"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jPIXZD6wUHWh"},"source":["### 4.1 Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5mOPgxDEUHWh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7c3b403c-835c-42c5-8c87-b1e218af1acf","executionInfo":{"status":"ok","timestamp":1744523368022,"user_tz":-480,"elapsed":2887424,"user":{"displayName":"Wong Matthew","userId":"12913150185447736015"}}},"source":["# Dictionary to store all results\n","lr_result = []\n","\n","# Logistic Regression parameters after grid search\n","# larger c to reduce regularization, minimize the diverge issue\n","# liblinear's performance is better than saga\n","# large max interation is used to minimize the diverge issue\n","lr_param_grid = {\n","    'C': [100],\n","    'solver': ['liblinear'],\n","    'max_iter': [2000]\n","}\n","\n","# For BoW features\n","print(\"Training Logistic Regression with BoW features...\")\n","lr_bow = grid_search_model(LogisticRegression(), lr_param_grid, X_bow_train, y_train, \"Logistic Regression (BoW)\")\n","lr_bow_result = train_evaluate_model(lr_bow, X_bow_train, X_bow_val, y_train, y_val, \"Logistic Regression\", \"BoW\")\n","lr_result.append(lr_bow_result)\n","\n","# For TF-IDF features\n","print(\"\\nTraining Logistic Regression with TF-IDF features...\")\n","lr_tfidf = grid_search_model(LogisticRegression(), lr_param_grid, X_tfidf_train, y_train, \"Logistic Regression (TF-IDF)\")\n","lr_tfidf_result = train_evaluate_model(lr_tfidf, X_tfidf_train, X_tfidf_val, y_train, y_val, \"Logistic Regression\", \"TF-IDF\")\n","lr_result.append(lr_tfidf_result)\n","\n","# For Word2Vec features\n","print(\"\\nTraining Logistic Regression with Word2Vec features...\")\n","lr_w2v = grid_search_model(LogisticRegression(), lr_param_grid, X_word2vec_train, y_train, \"Logistic Regression (Word2Vec)\")\n","lr_w2v_result = train_evaluate_model(lr_w2v, X_word2vec_train, X_word2vec_val, y_train, y_val, \"Logistic Regression\", \"Word2Vec\")\n","lr_result.append(lr_w2v_result)\n","\n","# For GloVe features\n","print(\"\\nTraining Logistic Regression with GloVe features...\")\n","lr_glove = grid_search_model(LogisticRegression(), lr_param_grid, X_glove_train, y_train, \"Logistic Regression (GloVe)\")\n","lr_glove_result = train_evaluate_model(lr_glove, X_glove_train, X_glove_val, y_train, y_val, \"Logistic Regression\", \"GloVe\")\n","lr_result.append(lr_glove_result)\n","\n","# For BERT features\n","print(\"\\nTraining Logistic Regression with BERT features...\")\n","lr_bert = grid_search_model(LogisticRegression(), lr_param_grid, X_bert_train, y_train, \"Logistic Regression (BERT)\")\n","lr_bert_result = train_evaluate_model(lr_bert, X_bert_train, X_bert_val, y_train, y_val, \"Logistic Regression\", \"BERT\")\n","lr_result.append(lr_bert_result)\n","\n","# Save the results to a file\n","lr_results_file = os.path.join(results_dir, 'logistic_regression_results.pkl')\n","with open(lr_results_file, 'wb') as f:\n","    pickle.dump(lr_result, f)\n","print(f\"Logistic Regression results saved to {lr_results_file}\")\n","\n","# Find the best performing model\n","best_svm_model = max(svm_result, key=lambda x: x['f1_score'])\n","best_svm_model_file = os.path.join(models_dir, f\"best_svm_{best_svm_model['feature_name'].lower()}.pkl\")\n","with open(best_svm_model_file, 'wb') as f:\n","    pickle.dump(best_svm_model['model'], f)\n","print(f\"Best SVM model saved to {best_svm_model_file}\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Training Logistic Regression with BoW features...\n","Fitting 5 folds for each of 1 candidates, totalling 5 fits\n","Best parameters for Logistic Regression (BoW): {'C': 100, 'max_iter': 2000, 'solver': 'liblinear'}\n","Best cross-validation score: 0.7674\n","Logistic Regression with BoW - Accuracy: 0.7573, F1 Score: 0.7574\n","\n","Training Logistic Regression with TF-IDF features...\n","Fitting 5 folds for each of 1 candidates, totalling 5 fits\n","Best parameters for Logistic Regression (TF-IDF): {'C': 100, 'max_iter': 2000, 'solver': 'liblinear'}\n","Best cross-validation score: 0.7641\n","Logistic Regression with TF-IDF - Accuracy: 0.7554, F1 Score: 0.7556\n","\n","Training Logistic Regression with Word2Vec features...\n","Fitting 5 folds for each of 1 candidates, totalling 5 fits\n","Best parameters for Logistic Regression (Word2Vec): {'C': 100, 'max_iter': 2000, 'solver': 'liblinear'}\n","Best cross-validation score: 0.4074\n","Logistic Regression with Word2Vec - Accuracy: 0.4119, F1 Score: 0.4132\n","\n","Training Logistic Regression with GloVe features...\n","Fitting 5 folds for each of 1 candidates, totalling 5 fits\n","Best parameters for Logistic Regression (GloVe): {'C': 100, 'max_iter': 2000, 'solver': 'liblinear'}\n","Best cross-validation score: 0.3993\n","Logistic Regression with GloVe - Accuracy: 0.4051, F1 Score: 0.4066\n","\n","Training Logistic Regression with BERT features...\n","Fitting 5 folds for each of 1 candidates, totalling 5 fits\n","Best parameters for Logistic Regression (BERT): {'C': 100, 'max_iter': 2000, 'solver': 'liblinear'}\n","Best cross-validation score: 0.5727\n","Logistic Regression with BERT - Accuracy: 0.5785, F1 Score: 0.5799\n","Logistic Regression model saved to /content/drive/MyDrive/Colab Notebooks/is5126/final-project/models/logistic_regression_bow.pkl\n","Logistic Regression model saved to /content/drive/MyDrive/Colab Notebooks/is5126/final-project/models/logistic_regression_tf-idf.pkl\n","Logistic Regression model saved to /content/drive/MyDrive/Colab Notebooks/is5126/final-project/models/logistic_regression_word2vec.pkl\n","Logistic Regression model saved to /content/drive/MyDrive/Colab Notebooks/is5126/final-project/models/logistic_regression_glove.pkl\n","Logistic Regression model saved to /content/drive/MyDrive/Colab Notebooks/is5126/final-project/models/logistic_regression_bert.pkl\n"]}]},{"cell_type":"markdown","metadata":{"id":"0KP-AjUxUHWh"},"source":["### Support Vector Machine (SVM)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eoj0JDmXUHWh"},"source":["# Dictionary to store all results\n","svm_result = []\n","\n","# SVM parameters after grid search\n","svm_param_grid = {\n","    'C': [1],\n","    'kernel': ['linear'],\n","    'gamma': ['scale']\n","}\n","\n","# For BoW features\n","print(\"Training SVM with BoW features...\")\n","svm_bow = grid_search_model(SVC(), svm_param_grid, X_bow_train, y_train, \"SVM (BoW)\")\n","svm_bow_result = train_evaluate_model(svm_bow, X_bow_train, X_bow_val, y_train, y_val, \"SVM\", \"BoW\")\n","svm_result.append(svm_bow_result)\n","\n","# For TF-IDF features\n","print(\"\\nTraining SVM with TF-IDF features...\")\n","svm_tfidf = grid_search_model(SVC(), svm_param_grid, X_tfidf_train, y_train, \"SVM (TF-IDF)\")\n","svm_tfidf_result = train_evaluate_model(svm_tfidf, X_tfidf_train, X_tfidf_val, y_train, y_val, \"SVM\", \"TF-IDF\")\n","svm_result.append(svm_tfidf_result)\n","\n","# For Word2Vec features\n","print(\"\\nTraining SVM with Word2Vec features...\")\n","svm_w2v = grid_search_model(SVC(), svm_param_grid, X_word2vec_train, y_train, \"SVM (Word2Vec)\")\n","svm_w2v_result = train_evaluate_model(svm_w2v, X_word2vec_train, X_word2vec_val, y_train, y_val, \"SVM\", \"Word2Vec\")\n","svm_result.append(svm_w2v_result)\n","\n","# For GloVe features\n","print(\"\\nTraining SVM with GloVe features...\")\n","svm_glove = grid_search_model(SVC(), svm_param_grid, X_glove_train, y_train, \"SVM (GloVe)\")\n","svm_glove_result = train_evaluate_model(svm_glove, X_glove_train, X_glove_val, y_train, y_val, \"SVM\", \"GloVe\")\n","svm_result.append(svm_glove_result)\n","\n","# For BERT features\n","print(\"\\nTraining SVM with BERT features...\")\n","svm_bert = grid_search_model(SVC(), svm_param_grid, X_bert_train, y_train, \"SVM (BERT)\")\n","svm_bert_result = train_evaluate_model(svm_bert, X_bert_train, X_bert_val, y_train, y_val, \"SVM\", \"BERT\")\n","svm_result.append(svm_bert_result)\n","\n","\n","# Save the results to a file\n","svm_results_file = os.path.join(results_dir, 'svm_results.pkl')\n","with open(svm_results_file, 'wb') as f:\n","    pickle.dump(svm_result, f)\n","print(f\"SVM results saved to {svm_results_file}\")\n","\n","# Find the best performing model\n","best_svm_model = max(svm_result, key=lambda x: x['f1_score'])\n","best_svm_model_file = os.path.join(models_dir, f\"best_svm_{best_svm_model['feature_name'].lower()}.pkl\")\n","with open(best_svm_model_file, 'wb') as f:\n","    pickle.dump(best_svm_model['model'], f)\n","print(f\"Best SVM model saved to {best_svm_model_file}\")"],"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VomylFvoUHWh"},"source":["### Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gd2f2a4zUHWh"},"source":["rf_result = []\n","\n","# RF parameters after grid search\n","best_params = {\n","    'n_estimators': 200,\n","    'max_depth': None,\n","    'min_samples_split': 2,\n","    'min_samples_leaf': 1\n","}\n","\n","# BoW features\n","print(\"Training Random Forest with BoW features...\")\n","rf_model_bow = RandomForestClassifier(\n","    n_estimators=best_params['n_estimators'],\n","    max_depth=best_params['max_depth'],\n","    min_samples_split=best_params['min_samples_split'],\n","    min_samples_leaf=best_params['min_samples_leaf'],\n","    random_state=42,\n","    n_jobs=-1\n",")\n","rf_bow_result = train_evaluate_model(\n","    model=rf_model_bow,\n","    X_train=X_bow_train,\n","    X_val=X_bow_val,be\n","    y_train=y_train,\n","    y_val=y_val,\n","    model_name=\"Random Forest\",\n","    feature_name=\"BoW\"\n",")\n","rf_result.append(rf_bow_result)\n","\n","\n","# TF-IDF features\n","print(\"\\nTraining Random Forest with TF-IDF features...\")\n","rf_model_tfidf = RandomForestClassifier(\n","    n_estimators=best_params['n_estimators'],\n","    max_depth=best_params['max_depth'],\n","    min_samples_split=best_params['min_samples_split'],\n","    min_samples_leaf=best_params['min_samples_leaf'],\n","    random_state=42,\n","    n_jobs=-1\n",")\n","rf_tfidf_result = train_evaluate_model(\n","    model=rf_model_tfidf,\n","    X_train=X_tfidf_train,\n","    X_val=X_tfidf_val,\n","    y_train=y_train,\n","    y_val=y_val,\n","    model_name=\"Random Forest\",\n","    feature_name=\"TF-IDF\"\n",")\n","rf_result.append(rf_tfidf_result)\n","\n","\n","# Word2Vec features\n","print(\"\\nTraining Random Forest with Word2Vec features...\")\n","rf_model_w2v = RandomForestClassifier(\n","    n_estimators=best_params['n_estimators'],\n","    max_depth=best_params['max_depth'],\n","    min_samples_split=best_params['min_samples_split'],\n","    min_samples_leaf=best_params['min_samples_leaf'],\n","    random_state=42,\n","    n_jobs=-1\n",")\n","rf_w2v_result = train_evaluate_model(\n","    model=rf_model_w2v,\n","    X_train=X_word2vec_train,\n","    X_val=X_word2vec_val,\n","    y_train=y_train,\n","    y_val=y_val,\n","    model_name=\"Random Forest\",\n","    feature_name=\"Word2Vec\"\n",")\n","rf_result.append(rf_w2v_result)\n","\n","\n","# GloVe features\n","print(\"\\nTraining Random Forest with GloVe features...\")\n","rf_model_glove = RandomForestClassifier(\n","    n_estimators=best_params['n_estimators'],\n","    max_depth=best_params['max_depth'],\n","    min_samples_split=best_params['min_samples_split'],\n","    min_samples_leaf=best_params['min_samples_leaf'],\n","    random_state=42,\n","    n_jobs=-1\n",")\n","rf_glove_result = train_evaluate_model(\n","    model=rf_model_glove,\n","    X_train=X_glove_train,\n","    X_val=X_glove_val,\n","    y_train=y_train,\n","    y_val=y_val,\n","    model_name=\"Random Forest\",\n","    feature_name=\"GloVe\"\n",")\n","rf_result.append(rf_glove_result)\n","\n","\n","# BERT features\n","print(\"\\nTraining Random Forest with BERT features...\")\n","rf_model_bert = RandomForestClassifier(\n","    n_estimators=best_params['n_estimators'],\n","    max_depth=best_params['max_depth'],\n","    min_samples_split=best_params['min_samples_split'],\n","    min_samples_leaf=best_params['min_samples_leaf'],\n","    random_state=42,\n","    n_jobs=-1\n",")\n","rf_bert_result = train_evaluate_model(\n","    model=rf_model_bert,\n","    X_train=X_bert_train,\n","    X_val=X_bert_val,\n","    y_train=y_train,\n","    y_val=y_val,\n","    model_name=\"Random Forest\",\n","    feature_name=\"BERT\"\n",")\n","rf_result.append(rf_bert_result)\n","\n","print(\"\\nFinished training Random Forest models for all feature types.\")\n","\n","# Save the results to a file\n","rf_results_file = os.path.join(results_dir, 'rf_results.pkl')\n","with open(rf_results_file, 'wb') as f:\n","    pickle.dump(rf_result, f)\n","print(f\"Random Forest results saved to {rf_results_file}\")\n","\n","# Find the best performing model\n","best_rf_model = max(rf_result, key=lambda x: x['f1_score'])\n","best_rf_model_file = os.path.join(models_dir, f\"best_rf_{best_rf_model['feature_name'].lower()}.pkl\")\n","with open(best_rf_model_file, 'wb') as f:\n","    pickle.dump(best_rf_model['model'], f)\n","print(f\"Best Random Forest model saved to {best_rf_model_file}\")"],"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PDEBMFBDUHWi"},"source":["## Deep Learning Models"]},{"cell_type":"markdown","metadata":{"id":"u-qUvAIvUHWi"},"source":["### BiLSTM Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qI6nOv7NUHWi"},"source":["# Define BiLSTM model\n","class BiLSTMClassifier(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, dropout):\n","        super(BiLSTMClassifier, self).__init__()\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=n_layers, bidirectional=True, dropout=dropout, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","\n","    def forward(self, text):\n","        # text shape: [batch size, input dim]\n","        # We need to add sequence length dimension for LSTM\n","        text = text.unsqueeze(1)  # Now: [batch size, 1, input dim]\n","\n","        output, (hidden, cell) = self.lstm(text)\n","\n","        # Concatenate the final forward and backward hidden states\n","        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n","\n","        return self.fc(hidden)\n","\n","# Dataset class for PyTorch\n","class EmbeddingDataset(Dataset):\n","    def __init__(self, embeddings, labels):\n","        self.embeddings = torch.FloatTensor(embeddings)\n","        # Convert string labels to numeric using the saved encoder\n","        numeric_labels = label_encoder.transform(labels)\n","        self.labels = torch.LongTensor(numeric_labels)\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        return self.embeddings[idx], self.labels[idx]\n","\n","# Function to train BiLSTM\n","def train_bilstm(X_train, X_val, y_train, y_val, embedding_type, n_epochs=10):\n","    # Check if CUDA is available\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Using device: {device}\")\n","\n","    # Create datasets and data loaders\n","    train_dataset = EmbeddingDataset(X_train, y_train)\n","    val_dataset = EmbeddingDataset(X_val, y_val)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=64)\n","\n","    # Create model\n","    input_dim = X_train.shape[1]\n","    hidden_dim = 128\n","    output_dim = len(np.unique(y_train))\n","    n_layers = 2\n","    dropout = 0.5\n","\n","    model = BiLSTMClassifier(input_dim, hidden_dim, output_dim, n_layers, dropout)\n","    model = model.to(device)\n","\n","    # Define optimizer and loss function\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # Training loop\n","    best_val_loss = float('inf')\n","    best_model = None\n","\n","    for epoch in range(n_epochs):\n","        # Training\n","        model.train()\n","        train_loss = 0\n","        train_acc = 0\n","\n","        for embeddings, labels in train_loader:\n","            embeddings, labels = embeddings.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            predictions = model(embeddings)\n","            loss = criterion(predictions, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            _, predicted = torch.max(predictions, 1)\n","            train_acc += (predicted == labels).sum().item() / len(labels)\n","\n","        train_loss /= len(train_loader)\n","        train_acc /= len(train_loader)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0\n","        val_acc = 0\n","\n","        with torch.no_grad():\n","            for embeddings, labels in val_loader:\n","                embeddings, labels = embeddings.to(device), labels.to(device)\n","                predictions = model(embeddings)\n","                loss = criterion(predictions, labels)\n","\n","                val_loss += loss.item()\n","                _, predicted = torch.max(predictions, 1)\n","                val_acc += (predicted == labels).sum().item() / len(labels)\n","\n","        val_loss /= len(val_loader)\n","        val_acc /= len(val_loader)\n","\n","        print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","        # Save best model\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_model = model.state_dict().copy()\n","\n","    # Load best model\n","    model.load_state_dict(best_model)\n","\n","    # Evaluate on validation set\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for embeddings, labels in val_loader:\n","            embeddings, labels = embeddings.to(device), labels.to(device)\n","            predictions = model(embeddings)\n","            _, predicted = torch.max(predictions, 1)\n","\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","    print(f\"BiLSTM with {embedding_type} - Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n","\n","    # Save model\n","    torch.save(model.state_dict(), os.path.join(models_dir, f'bilstm_{embedding_type.lower()}.pt'))\n","\n","    return {\n","        'model': model,\n","        'accuracy': accuracy,\n","        'f1_score': f1,\n","        'model_name': 'BiLSTM',\n","        'feature_name': embedding_type\n","    }"],"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yawOTybqUHWi"},"source":["# Train BiLSTM with Word2Vec embeddings\n","print(\"Training BiLSTM with Word2Vec embeddings...\")\n","bilstm_w2v_result = train_bilstm(X_word2vec_train, X_word2vec_val, y_train, y_val, \"Word2Vec\")\n","\n","# Train BiLSTM with GloVe embeddings\n","print(\"\\nTraining BiLSTM with GloVe embeddings...\")\n","bilstm_glove_result = train_bilstm(X_glove_train, X_glove_val, y_train, y_val, \"GloVe\")\n","\n","# Train BiLSTM with BERT embeddings\n","print(\"\\nTraining BiLSTM with BERT embeddings...\")\n","bilstm_bert_result = train_bilstm(X_bert_train, X_bert_val, y_train, y_val, \"BERT\")"],"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NXrYTSE4UHWi"},"source":["### Hyperparameter Tuning with Optuna for BiLSTM (BERT Embeddings only)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CwsTY4iZUHWi"},"source":["# Hyperparameter tuning for BiLSTM using Optuna\n","def objective(trial, X_train, X_val, y_train, y_val):\n","    # Define hyperparameters to optimize\n","    hidden_dim = trial.suggest_int('hidden_dim', 64, 256, step=64)\n","    n_layers = trial.suggest_int('n_layers', 1, 3)\n","    dropout = trial.suggest_float('dropout', 0.1, 0.5, step=0.1) if n_layers > 1 else 0.0\n","    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n","    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n","\n","    # Check if CUDA is available\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # Create datasets and data loaders\n","    train_dataset = EmbeddingDataset(X_train, y_train)\n","    val_dataset = EmbeddingDataset(X_val, y_val)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","    # Create model\n","    input_dim = X_train.shape[1]\n","    output_dim = len(np.unique(y_train))\n","\n","    model = BiLSTMClassifier(input_dim, hidden_dim, output_dim, n_layers, dropout)\n","    model = model.to(device)\n","\n","    # Define optimizer and loss function\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # Early stopping parameters\n","    patience = 3\n","    best_val_loss = float('inf')\n","    counter = 0\n","\n","    # Training loop\n","    n_epochs = 10  # Max epochs for tuning\n","\n","    for epoch in range(n_epochs):\n","        # Training\n","        model.train()\n","        train_loss = 0\n","\n","        for embeddings, labels in train_loader:\n","            embeddings, labels = embeddings.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            predictions = model(embeddings)\n","            loss = criterion(predictions, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0\n","        val_acc = 0\n","        all_preds = []\n","        all_labels = []\n","\n","        with torch.no_grad():\n","            for embeddings, labels in val_loader:\n","                embeddings, labels = embeddings.to(device), labels.to(device)\n","                predictions = model(embeddings)\n","                loss = criterion(predictions, labels)\n","\n","                val_loss += loss.item()\n","                _, predicted = torch.max(predictions, 1)\n","                all_preds.extend(predicted.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","        val_loss /= len(val_loader)\n","        f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","        # Report score to Optuna\n","        trial.report(f1, epoch)\n","\n","        # Early stopping\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            counter = 0\n","        else:\n","            counter += 1\n","            if counter >= patience:\n","                break\n","\n","        # Handle pruning\n","        if trial.should_prune():\n","            raise optuna.exceptions.TrialPruned()\n","\n","    return f1\n","\n","# Run optimization for BiLSTM with Word2Vec embeddings\n","def optimize_bilstm(X_train, X_val, y_train, y_val, embedding_type, n_trials=20):\n","    print(f\"\\nOptimizing BiLSTM with {embedding_type} embeddings...\")\n","\n","    # Create study\n","    study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n","\n","    # Run optimization\n","    study.optimize(lambda trial: objective(trial, X_train, X_val, y_train, y_val), n_trials=n_trials)\n","\n","    # Print results\n","    print(f\"\\nBest trial for BiLSTM with {embedding_type}:\")\n","    trial = study.best_trial\n","    print(f\"  F1 Score: {trial.value:.4f}\")\n","    print(\"  Params:\")\n","    for key, value in trial.params.items():\n","        print(f\"    {key}: {value}\")\n","\n","    return study.best_params\n","\n","best_params_bert = optimize_bilstm(X_bert_train, X_bert_val, y_train, y_val, \"BERT\", n_trials=10)"],"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SZXKrDkyUHWi"},"source":["# Function to train BiLSTM with optimized hyperparameters\n","def train_bilstm_optimized(X_train, X_val, y_train, y_val, embedding_type, params, n_epochs=15):\n","    # Check if CUDA is available\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Using device: {device}\")\n","\n","    # Extract parameters\n","    hidden_dim = params['hidden_dim']\n","    n_layers = params['n_layers']\n","    dropout = params['dropout'] if n_layers > 1 else 0.0\n","    learning_rate = params['learning_rate']\n","    batch_size = params['batch_size']\n","\n","    # Create datasets and data loaders\n","    train_dataset = EmbeddingDataset(X_train, y_train)\n","    val_dataset = EmbeddingDataset(X_val, y_val)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","    # Create model\n","    input_dim = X_train.shape[1]\n","    output_dim = len(np.unique(y_train))\n","\n","    model = BiLSTMClassifier(input_dim, hidden_dim, output_dim, n_layers, dropout)\n","    model = model.to(device)\n","\n","    # Define optimizer and loss function\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # Training loop\n","    best_val_loss = float('inf')\n","    best_model = None\n","\n","    for epoch in range(n_epochs):\n","        # Training\n","        model.train()\n","        train_loss = 0\n","        train_acc = 0\n","\n","        for embeddings, labels in train_loader:\n","            embeddings, labels = embeddings.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            predictions = model(embeddings)\n","            loss = criterion(predictions, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            _, predicted = torch.max(predictions, 1)\n","            train_acc += (predicted == labels).sum().item() / len(labels)\n","\n","        train_loss /= len(train_loader)\n","        train_acc /= len(train_loader)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0\n","        val_acc = 0\n","\n","        with torch.no_grad():\n","            for embeddings, labels in val_loader:\n","                embeddings, labels = embeddings.to(device), labels.to(device)\n","                predictions = model(embeddings)\n","                loss = criterion(predictions, labels)\n","\n","                val_loss += loss.item()\n","                _, predicted = torch.max(predictions, 1)\n","                val_acc += (predicted == labels).sum().item() / len(labels)\n","\n","        val_loss /= len(val_loader)\n","        val_acc /= len(val_loader)\n","\n","        print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","        # Save best model\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_model = model.state_dict().copy()\n","\n","    # Load best model\n","    model.load_state_dict(best_model)\n","\n","    # Evaluate on validation set\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for embeddings, labels in val_loader:\n","            embeddings, labels = embeddings.to(device), labels.to(device)\n","            predictions = model(embeddings)\n","            _, predicted = torch.max(predictions, 1)\n","\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","    print(f\"Optimized BiLSTM with {embedding_type} - Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n","\n","    # Save model\n","    torch.save(model.state_dict(), os.path.join(models_dir, f'bilstm_{embedding_type.lower()}_optimized.pt'))\n","\n","    return {\n","        'model': model,\n","        'accuracy': accuracy,\n","        'f1_score': f1,\n","        'model_name': 'BiLSTM (Optimized)',\n","        'feature_name': embedding_type\n","    }"],"outputs":[]},{"cell_type":"code","source":["bilstm_result = []\n","\n","# Best param found by optuna\n","best_params_bert = {'hidden_dim': 256, 'n_layers': 1, 'learning_rate': 0.0012202088236504737, 'batch_size': 64}\n","\n","# Train BiLSTM with BERT embeddings\n","print(\"\\nTraining BiLSTM with BERT embeddings...\")\n","bilstm_bert_result = train_bilstm_optimized(X_bert_train, X_bert_val, y_train, y_val, \"BERT\", best_params_bert)\n","bilstm_result.append(bilstm_bert_result)\n","\n","# Save the results to a file\n","bilstm_results_file = os.path.join(results_dir, 'bilstm_results.pkl')\n","with open(bilstm_results_file, 'wb') as f:\n","  pickle.dump(bilstm_result, f)\n","print(f\"BiLSTM results saved to {bilstm_results_file}\")\n","\n","# Find the best performing model\n","best_bilstm_model = max(bilstm_result, key=lambda x: x['f1_score'])\n","best_bilstm_model_file = os.path.join(models_dir, f\"best_bilstm_{best_bilstm_model['feature_name'].lower()}.pt\")\n","torch.save(best_bilstm_model['model'].state_dict(), best_bilstm_model_file)\n","print(f\"Best BiLSTM model saved to {best_bilstm_model_file}\")"],"metadata":{"id":"H3VDgkvkBqfy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I9s0IWOqUHWj"},"source":["### BERT Fine-tuning with Hyperparameter Optimization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BT8JxUORUHWj"},"source":["# BERT Dataset class\n","class BERTDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len=128):\n","        self.texts = texts\n","        # Convert string labels to numeric using the saved encoder\n","        numeric_labels = label_encoder.transform(labels)\n","        self.labels = torch.LongTensor(numeric_labels)\n","\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx])\n","        label = self.labels[idx]\n","\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=True,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'token_type_ids': encoding['token_type_ids'].flatten(),\n","            'label': torch.tensor(label, dtype=torch.long)\n","        }"],"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZGleFW1UHWj"},"source":["def train_epoch(model, data_loader, optimizer, device):\n","    model.train()\n","    total_loss = 0\n","    total_preds = []\n","    total_labels = []\n","\n","    for batch in data_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        token_type_ids = batch['token_type_ids'].to(device)\n","        labels = batch['label'].to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            labels=labels\n","        )\n","\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","\n","        _, preds = torch.max(outputs.logits, dim=1)\n","        total_preds.extend(preds.cpu().numpy())\n","        total_labels.extend(labels.cpu().numpy())\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    avg_loss = total_loss / len(data_loader)\n","    f1 = f1_score(total_labels, total_preds, average='weighted')\n","    accuracy = accuracy_score(total_labels, total_preds)\n","\n","    return avg_loss, accuracy, f1\n","\n","def evaluate(model, data_loader, device):\n","    model.eval()\n","    total_loss = 0\n","    total_preds = []\n","    total_labels = []\n","\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            token_type_ids = batch['token_type_ids'].to(device)\n","            labels = batch['label'].to(device)\n","\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                token_type_ids=token_type_ids,\n","                labels=labels\n","            )\n","\n","            total_loss += outputs.loss.item()\n","            _, preds = torch.max(outputs.logits, dim=1)\n","            total_preds.extend(preds.cpu().numpy())\n","            total_labels.extend(labels.cpu().numpy())\n","\n","    avg_loss = total_loss / len(data_loader)\n","    f1 = f1_score(total_labels, total_preds, average='weighted')\n","    accuracy = accuracy_score(total_labels, total_preds)\n","\n","    return avg_loss, accuracy, f1"],"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wfzYT0RIUHWj"},"source":["from tqdm.auto import tqdm\n","\n","def optimize_bert(tweets_train, tweets_val, y_train, y_val, num_labels, n_trials=3):\n","    def objective(trial):\n","        print(f\"\\nStarting trial {trial.number + 1}/{n_trials}\")\n","\n","        # Simplified hyperparameter space\n","        params = {\n","            'learning_rate': trial.suggest_float('learning_rate', 1e-5, 5e-5, log=True),\n","            'batch_size': trial.suggest_categorical('batch_size', [16, 32]),\n","            'max_len': 64,  # Fixed to shorter sequence length\n","            'weight_decay': trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True)\n","        }\n","\n","        # Initialize model and tokenizer\n","        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","        model = BertForSequenceClassification.from_pretrained(\n","            'bert-base-uncased',\n","            num_labels=num_labels\n","        )\n","\n","        # Create datasets using full data\n","        train_dataset = BERTDataset(tweets_train, y_train, tokenizer, max_len=params['max_len'])\n","        val_dataset = BERTDataset(tweets_val, y_val, tokenizer, max_len=params['max_len'])\n","\n","        train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n","        val_loader = DataLoader(val_dataset, batch_size=params['batch_size'])\n","\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        model = model.to(device)\n","        optimizer = AdamW(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n","\n","        best_val_f1 = 0\n","        epochs = 2  # Reduced number of epochs\n","\n","        for epoch in range(epochs):\n","            # Training\n","            model.train()\n","            train_progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} - Training')\n","            for batch in train_progress:\n","                optimizer.zero_grad()\n","                inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n","                labels = batch['label'].to(device)\n","\n","                outputs = model(**inputs, labels=labels)\n","                loss = outputs.loss\n","                loss.backward()\n","                optimizer.step()\n","\n","                train_progress.set_postfix({'loss': f'{loss.item():.4f}'})\n","\n","            # Validation\n","            model.eval()\n","            val_preds = []\n","            val_labels = []\n","\n","            with torch.no_grad():\n","                for batch in tqdm(val_loader, desc='Validation'):\n","                    inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n","                    labels = batch['label'].to(device)\n","                    outputs = model(**inputs)\n","                    _, preds = torch.max(outputs.logits, dim=1)\n","                    val_preds.extend(preds.cpu().numpy())\n","                    val_labels.extend(labels.cpu().numpy())\n","\n","            val_f1 = f1_score(val_labels, val_preds, average='weighted')\n","            print(f'Epoch {epoch+1}/{epochs} - Validation F1: {val_f1:.4f}')\n","\n","            if val_f1 > best_val_f1:\n","                best_val_f1 = val_f1\n","\n","            # Early pruning\n","            trial.report(val_f1, epoch)\n","            if trial.should_prune():\n","                raise optuna.exceptions.TrialPruned()\n","\n","        return best_val_f1\n","\n","    # Create study with aggressive pruning\n","    study = optuna.create_study(\n","        direction='maximize',\n","        pruner=optuna.pruners.MedianPruner(\n","            n_startup_trials=1,\n","            n_warmup_steps=5,\n","            interval_steps=1\n","        )\n","    )\n","\n","    study.optimize(objective, n_trials=n_trials)\n","\n","    print(\"\\nOptimization completed!\")\n","    print(f\"Best trial F1 score: {study.best_trial.value:.4f}\")\n","    print(\"Best parameters:\", study.best_params)\n","\n","    return study.best_params"],"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BEt017v7UHWj"},"source":["def train_bert(tweets_train, tweets_val, y_train, y_val, params, num_labels):\n","    # Initialize tokenizer and model\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    model = BertForSequenceClassification.from_pretrained(\n","        'bert-base-uncased',\n","        num_labels=num_labels\n","    )\n","\n","    # Create datasets\n","    train_dataset = BERTDataset(tweets_train, y_train, tokenizer)\n","    val_dataset = BERTDataset(tweets_val, y_val, tokenizer)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=params['batch_size'])\n","\n","    # Setup training\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","    optimizer = AdamW(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])\n","\n","    # Training loop\n","    best_val_f1 = 0\n","    best_model = None\n","    epochs = 3  # Reduced number of epochs for faster training\n","\n","    for epoch in range(epochs):\n","        print(f'Epoch {epoch + 1}/{epochs}')\n","\n","        # Train\n","        model.train()\n","        train_loss = 0\n","        train_preds = []\n","        train_labels = []\n","\n","        train_progress = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs} - Training')\n","        for batch in train_progress:\n","            optimizer.zero_grad()\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            token_type_ids = batch['token_type_ids'].to(device)\n","            labels = batch['label'].to(device)\n","\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                token_type_ids=token_type_ids,\n","                labels=labels\n","            )\n","\n","            loss = outputs.loss\n","            train_loss += loss.item()\n","            loss.backward()\n","            optimizer.step()\n","\n","            _, preds = torch.max(outputs.logits, dim=1)\n","            train_preds.extend(preds.cpu().numpy())\n","            train_labels.extend(labels.cpu().numpy())\n","\n","            train_progress.set_postfix({'loss': f'{loss.item():.4f}'})\n","\n","        train_loss /= len(train_loader)\n","        train_f1 = f1_score(train_labels, train_preds, average='weighted')\n","        train_acc = accuracy_score(train_labels, train_preds)\n","        print(f'Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, F1: {train_f1:.4f}')\n","\n","        # Validate\n","        val_loss, val_acc, val_f1 = evaluate(model, val_loader, device)\n","        print(f'Val Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}')\n","\n","        if val_f1 > best_val_f1:\n","            best_val_f1 = val_f1\n","            best_model = model.state_dict().copy()\n","\n","    # Save best model\n","    model.load_state_dict(best_model)\n","    torch.save(model.state_dict(), os.path.join(models_dir, 'best_bert.pt'))\n","\n","    return {\n","        'model': model,\n","        'accuracy': val_acc,\n","        'f1_score': best_val_f1,\n","        'model_name': 'BERT',\n","        'feature_name': 'Fine-tuned'\n","    }"],"outputs":[]},{"cell_type":"code","source":["bert_result = []\n","\n","# Run BERT optimization and training\n","print(\"Starting BERT hyperparameter optimization...\")\n","best_params = optimize_bert(train_df['cleaned_content'], val_df['cleaned_content'], y_train, y_val, len(np.unique(y_train)), n_trials=1)\n","print(\"Best parameters:\", best_params)\n","\n","# Best param found\n","best_params_bert = {\n","    'learning_rate': 2.081063824861354e-05,\n","    'batch_size': 32,\n","    'weight_decay': 0.0019994876077473584\n","}\n","\n","print(\"\\nTraining BERT with optimized parameters...\")\n","bert_results = train_bert(train_df['cleaned_content'], val_df['cleaned_content'], y_train, y_val, best_params_bert, len(np.unique(y_train)))\n","bert_result.append(bert_results)\n","\n","print(f\"\\nBERT Training Complete - Validation F1: {bert_results['f1_score']:.4f}\")\n","\n","# Save the results to a file\n","bert_results_file = os.path.join(results_dir, 'bert_results.pkl')\n","with open(bert_results_file, 'wb') as f:\n","  pickle.dump(bert_result, f)\n","print(f\"BERT results saved to {bert_results_file}\")\n","\n","# Find the best performing model\n","best_bert_model = max(bert_result, key=lambda x: x['f1_score'])\n","best_bert_model_file = os.path.join(models_dir, f\"best_bert_{best_bert_model['feature_name'].lower()}.pt\")\n","torch.save(best_bert_model['model'].state_dict(), best_bert_model_file)\n","print(f\"Best BERT model saved to {best_bert_model_file}\")"],"metadata":{"id":"ebysI69BSJLA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DBfYqxVTUHWj"},"source":["## Compare Model Performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Yt-2nFAUHWj"},"source":["# Load results from saved files\n","with open(os.path.join(results_dir, 'logistic_regression_results.pkl'), 'rb') as f:\n","    lr_results = pickle.load(f)\n","\n","with open(os.path.join(results_dir, 'svm_results.pkl'), 'rb') as f:\n","    svm_results = pickle.load(f)\n","\n","with open(os.path.join(results_dir, 'rf_results.pkl'), 'rb') as f:\n","    rf_results = pickle.load(f)\n","\n","with open(os.path.join(results_dir, 'bilstm_results.pkl'), 'rb') as f:\n","    bilstm_results = pickle.load(f)\n","\n","with open(os.path.join(results_dir, 'bert_results.pkl'), 'rb') as f:\n","    bert_results = pickle.load(f)\n","\n","# Combine all results into a single list\n","results = lr_results + rf_results + bilstm_results + bert_results\n","results_df = pd.DataFrame([\n","    {'Model': r['model_name'],\n","     'Feature': r['feature_name'],\n","     'Accuracy': r['accuracy'],\n","     'F1 Score': r['f1_score']} for r in results\n","])\n","\n","# Sort by F1 score (descending)\n","results_df = results_df.sort_values(by='F1 Score', ascending=False).reset_index(drop=True)\n","\n","# Display results\n","print(\"Model Performance on Validation Set:\")\n","results_df"],"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T6toOOZBUHWj"},"source":["# Visualize results\n","plt.figure(figsize=(14, 8))\n","sns.barplot(x='Model', y='F1 Score', hue='Feature', data=results_df)\n","plt.title('Model Performance Comparison (F1 Score)', fontsize=15)\n","plt.xlabel('Model', fontsize=12)\n","plt.ylabel('F1 Score', fontsize=12)\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.savefig(os.path.join(results_dir, 'model_comparison.png'))\n","plt.show()"],"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dbcxQl-HUHWj"},"source":["## 7. Save Best Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOCaeH8YUHWj"},"source":["# Get top 3 performing models\n","top_models = results_df.head(3)\n","print(\"Top 3 performing models:\")\n","top_models"],"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W3Fp397CUHWj"},"source":["# Save results\n","results_df.to_csv(os.path.join(results_dir, 'model_comparison_results.csv'), index=False)\n","print(f\"Model comparison results saved to {os.path.join(results_dir, 'model_comparison_results.csv')}\")"],"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}