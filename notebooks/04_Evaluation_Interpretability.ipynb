{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis - Evaluation & Interpretability\n",
    "\n",
    "This notebook focuses on evaluating the trained models and interpreting their predictions:\n",
    "\n",
    "1. Load the best-performing models\n",
    "2. Evaluate models on the test set\n",
    "3. Generate classification reports and confusion matrices\n",
    "4. Apply SHAP values for model interpretability\n",
    "5. Compare feature importance across different models\n",
    "6. Analyze misclassified examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, roc_auc_score\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Interpretability\n",
    "import shap\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style='whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Models and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# For Google Colab, uncomment these lines to mount Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# features_dir = '/content/drive/MyDrive/path/to/features'\n",
    "# models_dir = '/content/drive/MyDrive/path/to/models'\n",
    "# results_dir = '/content/drive/MyDrive/path/to/results'\n",
    "\n",
    "# For local development\n",
    "features_dir = '../data/features'\n",
    "models_dir = '../models'\n",
    "results_dir = '../results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Load model comparison results\n",
    "model_results = pd.read_csv(os.path.join(results_dir, 'model_comparison_results.csv'))\n",
    "\n",
    "# Get top performing models\n",
    "top_models = model_results.head(3)\n",
    "print(\"Top 3 performing models:\")\n",
    "top_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Load test data\n",
    "# Load labels\n",
    "y = np.load(os.path.join(features_dir, 'labels.npy'))\n",
    "\n",
    "# Load features\n",
    "X_bow = sp.load_npz(os.path.join(features_dir, 'bow_features.npz'))\n",
    "X_tfidf = sp.load_npz(os.path.join(features_dir, 'tfidf_features.npz'))\n",
    "X_word2vec = np.load(os.path.join(features_dir, 'word2vec_features.npy'))\n",
    "X_glove = np.load(os.path.join(features_dir, 'glove_features.npy'))\n",
    "X_bert = np.load(os.path.join(features_dir, 'bert_features.npy'))\n",
    "\n",
    "print(\"Features loaded successfully.\")\n",
    "\n",
    "# Load the split indices if they were saved\n",
    "# Otherwise, recreate the splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(X, y, test_size=0.15, val_size=0.15, random_state=42):\n",
    "    # First split: training + validation vs test\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Second split: training vs validation\n",
    "    # Adjust validation size to be a percentage of the training + validation set\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=val_ratio, random_state=random_state, stratify=y_train_val\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Create train/val/test splits for each feature type\n",
    "# BoW features\n",
    "_, _, bow_test, _, _, y_test = split_data(X_bow, y)\n",
    "\n",
    "# TF-IDF features\n",
    "_, _, tfidf_test, _, _, _ = split_data(X_tfidf, y, random_state=42)\n",
    "\n",
    "# Word2Vec features\n",
    "_, _, w2v_test, _, _, _ = split_data(X_word2vec, y, random_state=42)\n",
    "\n",
    "# GloVe features\n",
    "_, _, glove_test, _, _, _ = split_data(X_glove, y, random_state=42)\n",
    "\n",
    "# BERT features\n",
    "_, _, bert_test, _, _, _ = split_data(X_bert, y, random_state=42)\n",
    "\n",
    "print(f\"Test set: {y_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Load label encoder\n",
    "with open(os.path.join(models_dir, 'label_encoder.pkl'), 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Display label encoding\n",
    "print(\"Label Encoding:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{i} -> {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Evaluate Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Function to load a saved sklearn model\n",
    "def load_sklearn_model(model_path):\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "# Function to load BiLSTM model\n",
    "def load_bilstm_model(model_path, input_dim):\n",
    "    # Define BiLSTM model class\n",
    "    class BiLSTMClassifier(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "            super(BiLSTMClassifier, self).__init__()\n",
    "            self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=n_layers, bidirectional=True, dropout=dropout, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "            \n",
    "        def forward(self, text):\n",
    "            # text shape: [batch size, input dim]\n",
    "            # We need to add sequence length dimension for LSTM\n",
    "            text = text.unsqueeze(1)  # Now: [batch size, 1, input dim]\n",
    "            \n",
    "            output, (hidden, cell) = self.lstm(text)\n",
    "            \n",
    "            # Concatenate the final forward and backward hidden states\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "            \n",
    "            return self.fc(hidden)\n",
    "    \n",
    "    # Create model with same architecture\n",
    "    hidden_dim = 128\n",
    "    output_dim = len(label_encoder.classes_)\n",
    "    n_layers = 2\n",
    "    dropout = 0.5\n",
    "    \n",
    "    model = BiLSTMClassifier(input_dim, hidden_dim, output_dim, n_layers, dropout)\n",
    "    \n",
    "    # Load weights\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, device\n",
    "\n",
    "# Function to evaluate model and generate report\n",
    "def evaluate_model(model, X_test, y_test, model_name, feature_name, is_bilstm=False, device=None):\n",
    "    if is_bilstm:\n",
    "        # Predict using BiLSTM\n",
    "        X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_test_tensor)\n",
    "            _, y_pred = torch.max(outputs, 1)\n",
    "            y_pred = y_pred.cpu().numpy()\n",
    "    else:\n",
    "        # Predict using sklearn model\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    report = classification_report(y_test, y_pred, target_names=label_encoder.classes_, output_dict=True)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} with {feature_name} - Test Set Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score (weighted): {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=label_encoder.classes_,\n",
    "                yticklabels=label_encoder.classes_)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix - {model_name} with {feature_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, f'confusion_matrix_{model_name.lower()}_{feature_name.lower()}.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'feature_name': feature_name,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'report': report,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'predictions': y_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Evaluate each top model\n",
    "test_results = []\n",
    "\n",
    "for _, row in top_models.iterrows():\n",
    "    model_name = row['Model']\n",
    "    feature_name = row['Feature']\n",
    "    \n",
    "    print(f\"\\nEvaluating {model_name} with {feature_name}...\")\n",
    "    \n",
    "    # Load the appropriate test set\n",
    "    if feature_name == 'BoW':\n",
    "        X_test_features = bow_test\n",
    "    elif feature_name == 'TF-IDF':\n",
    "        X_test_features = tfidf_test\n",
    "    elif feature_name == 'Word2Vec':\n",
    "        X_test_features = w2v_test\n",
    "    elif feature_name == 'GloVe':\n",
    "        X_test_features = glove_test\n",
    "    elif feature_name == 'BERT':\n",
    "        X_test_features = bert_test\n",
    "    \n",
    "    # Load the model\n",
    "    if model_name == 'BiLSTM':\n",
    "        model_path = os.path.join(models_dir, f'bilstm_{feature_name.lower()}.pt')\n",
    "        model, device = load_bilstm_model(model_path, X_test_features.shape[1])\n",
    "        result = evaluate_model(model, X_test_features, y_test, model_name, feature_name, is_bilstm=True, device=device)\n",
    "    else:\n",
    "        # Customize this part based on how you saved your models\n",
    "        model_path = os.path.join(models_dir, f'{model_name.lower()}_{feature_name.lower()}.pkl')\n",
    "        model = load_sklearn_model(model_path)\n",
    "        result = evaluate_model(model, X_test_features, y_test, model_name, feature_name)\n",
    "    \n",
    "    test_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Interpretation with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Function to interpret sklearn model with SHAP\n",
    "def interpret_sklearn_model(model, X_train, X_test, feature_names, model_name, feature_type):\n",
    "    print(f\"\\nGenerating SHAP values for {model_name} with {feature_type}...\")\n",
    "    \n",
    "    # For large sparse matrices, sample a subset for speed\n",
    "    if isinstance(X_train, sp.spmatrix) and X_train.shape[1] > 1000:\n",
    "        # Limit to top 1000 features\n",
    "        X_train_sample = X_train[:, :1000]\n",
    "        X_test_sample = X_test[:, :1000]\n",
    "        feature_names = feature_names[:1000] if feature_names is not None else None\n",
    "    else:\n",
    "        X_train_sample = X_train\n",
    "        X_test_sample = X_test\n",
    "    \n",
    "    # Convert sparse matrices to dense if needed\n",
    "    if isinstance(X_train_sample, sp.spmatrix):\n",
    "        X_train_sample = X_train_sample.toarray()\n",
    "    if isinstance(X_test_sample, sp.spmatrix):\n",
    "        X_test_sample = X_test_sample.toarray()\n",
    "    \n",
    "    # Sample for speed if necessary\n",
    "    if X_train_sample.shape[0] > 500:\n",
    "        indices = np.random.choice(X_train_sample.shape[0], 500, replace=False)\n",
    "        X_train_sample = X_train_sample[indices]\n",
    "    \n",
    "    # Use appropriate explainer\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        explainer = shap.KernelExplainer(model.predict_proba, X_train_sample)\n",
    "        # Sample test set for speed if necessary\n",
    "        if X_test_sample.shape[0] > 50:\n",
    "            test_indices = np.random.choice(X_test_sample.shape[0], 50, replace=False)\n",
    "            X_test_sample = X_test_sample[test_indices]\n",
    "        \n",
    "        shap_values = explainer.shap_values(X_test_sample)\n",
    "        \n",
    "        # Plot summary\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values, X_test_sample, feature_names=feature_names, show=False)\n",
    "        plt.title(f\"SHAP Summary Plot - {model_name} with {feature_type}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(results_dir, f'shap_summary_{model_name.lower()}_{feature_type.lower()}.png'))\n",
    "        plt.show()\n",
    "        \n",
    "        return shap_values\n",
    "    else:\n",
    "        # For models without predict_proba\n",
    "        explainer = shap.KernelExplainer(model.predict, X_train_sample)\n",
    "        if X_test_sample.shape[0] > 50:\n",
    "            test_indices = np.random.choice(X_test_sample.shape[0], 50, replace=False)\n",
    "            X_test_sample = X_test_sample[test_indices]\n",
    "            \n",
    "        shap_values = explainer.shap_values(X_test_sample)\n",
    "        \n",
    "        # Plot summary\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values, X_test_sample, feature_names=feature_names, show=False)\n",
    "        plt.title(f\"SHAP Summary Plot - {model_name} with {feature_type}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(results_dir, f'shap_summary_{model_name.lower()}_{feature_type.lower()}.png'))\n",
    "        plt.show()\n",
    "        \n",
    "        return shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Apply SHAP to the best models (only for sklearn models)\n",
    "for result in test_results:\n",
    "    if result['model_name'] != 'BiLSTM':  # Skip BiLSTM for now\n",
    "        model_name = result['model_name']\n",
    "        feature_name = result['feature_name']\n",
    "        \n",
    "        # Load the model\n",
    "        model_path = os.path.join(models_dir, f'{model_name.lower()}_{feature_name.lower()}.pkl')\n",
    "        model = load_sklearn_model(model_path)\n",
    "        \n",
    "        # Load appropriate features\n",
    "        if feature_name == 'BoW':\n",
    "            X_train_features, _, X_test_features, _, _, _ = split_data(X_bow, y)\n",
    "            # Load vectorizer to get feature names\n",
    "            with open(os.path.join(models_dir, 'bow_vectorizer.pkl'), 'rb') as f:\n",
    "                vectorizer = pickle.load(f)\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "        elif feature_name == 'TF-IDF':\n",
    "            X_train_features, _, X_test_features, _, _, _ = split_data(X_tfidf, y)\n",
    "            # Load vectorizer to get feature names\n",
    "            with open(os.path.join(models_dir, 'tfidf_vectorizer.pkl'), 'rb') as f:\n",
    "                vectorizer = pickle.load(f)\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "        elif feature_name == 'Word2Vec':\n",
    "            X_train_features, _, X_test_features, _, _, _ = split_data(X_word2vec, y)\n",
    "            feature_names = [f\"dim_{i}\" for i in range(X_train_features.shape[1])]\n",
    "        elif feature_name == 'GloVe':\n",
    "            X_train_features, _, X_test_features, _, _, _ = split_data(X_glove, y)\n",
    "            feature_names = [f\"dim_{i}\" for i in range(X_train_features.shape[1])]\n",
    "        elif feature_name == 'BERT':\n",
    "            X_train_features, _, X_test_features, _, _, _ = split_data(X_bert, y)\n",
    "            feature_names = [f\"dim_{i}\" for i in range(X_train_features.shape[1])]\n",
    "        \n",
    "        # Interpret model\n",
    "        interpret_sklearn_model(model, X_train_features, X_test_features, feature_names, model_name, feature_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Misclassified Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Load original dataset to examine misclassified examples\n",
    "dataset_path = '../data/cleaned_tweets.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "print(f\"Loaded cleaned dataset with shape: {df.shape}\")\n",
    "\n",
    "# Function to analyze misclassified examples\n",
    "def analyze_misclassifications(result, df, label_encoder):\n",
    "    model_name = result['model_name']\n",
    "    feature_name = result['feature_name']\n",
    "    y_pred = result['predictions']\n",
    "    \n",
    "    # Get test indices\n",
    "    _, _, _, _, _, test_indices = split_data(np.arange(len(df)), y)\n",
    "    \n",
    "    # Create DataFrame with test set results\n",
    "    test_df = df.iloc[test_indices].copy()\n",
    "    test_df['true_sentiment'] = label_encoder.inverse_transform(y_test)\n",
    "    test_df['predicted_sentiment'] = label_encoder.inverse_transform(y_pred)\n",
    "    test_df['correct'] = test_df['true_sentiment'] == test_df['predicted_sentiment']\n",
    "    \n",
    "    # Filter misclassified examples\n",
    "    misclassified = test_df[~test_df['correct']]\n",
    "    \n",
    "    print(f\"\\nMisclassified Examples for {model_name} with {feature_name}:\")\n",
    "    print(f\"Total misclassified: {len(misclassified)} out of {len(test_df)} ({len(misclassified)/len(test_df)*100:.2f}%)\")\n",
    "    \n",
    "    # Count by true sentiment\n",
    "    true_sentiment_counts = misclassified['true_sentiment'].value_counts()\n",
    "    print(\"\\nMisclassifications by True Sentiment:\")\n",
    "    for sentiment, count in true_sentiment_counts.items():\n",
    "        total_in_class = test_df[test_df['true_sentiment'] == sentiment].shape[0]\n",
    "        print(f\"{sentiment}: {count} out of {total_in_class} ({count/total_in_class*100:.2f}%)\")\n",
    "    \n",
    "    # Show confusion patterns\n",
    "    print(\"\\nConfusion Patterns:\")\n",
    "    confusion_counts = misclassified.groupby(['true_sentiment', 'predicted_sentiment']).size().reset_index()\n",
    "    confusion_counts.columns = ['True Sentiment', 'Predicted Sentiment', 'Count']\n",
    "    confusion_counts = confusion_counts.sort_values('Count', ascending=False)\n",
    "    print(confusion_counts)\n",
    "    \n",
    "    # Display some examples\n",
    "    print(\"\\nSample Misclassified Examples:\")\n",
    "    for i, (sentiment_pair, group) in enumerate(misclassified.groupby(['true_sentiment', 'predicted_sentiment'])):\n",
    "        true_sentiment, pred_sentiment = sentiment_pair\n",
    "        print(f\"\\nTrue: {true_sentiment}, Predicted: {pred_sentiment}\")\n",
    "        for _, row in group.head(2).iterrows():\n",
    "            print(f\"Tweet: {row['tweet']}\")\n",
    "            print(f\"Cleaned: {row['cleaned_tweet']}\")\n",
    "            print(\"-----\")\n",
    "        \n",
    "        if i >= 2:  # Limit to a few examples\n",
    "            break\n",
    "    \n",
    "    return misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Analyze misclassifications for the best model\n",
    "best_result = test_results[0]  # Assuming results are sorted by performance\n",
    "misclassified_examples = analyze_misclassifications(best_result, df, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Create summary table of test results\n",
    "test_summary = pd.DataFrame([\n",
    "    {'Model': r['model_name'], \n",
    "     'Feature': r['feature_name'], \n",
    "     'Accuracy': r['accuracy'], \n",
    "     'F1 Score': r['f1_score']} for r in test_results\n",
    "])\n",
    "\n",
    "print(\"Test Set Performance Summary:\")\n",
    "test_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Plot model comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='F1 Score', hue='Feature', data=test_summary)\n",
    "plt.title('Test Set Performance (F1 Score)', fontsize=15)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('F1 Score', fontsize=12)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'test_performance_comparison.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Findings and Insights\n",
    "\n",
    "Based on our evaluation, we can draw the following conclusions:\n",
    "\n",
    "1. **Best Model Performance**:\n",
    "   - The [MODEL] with [FEATURE] achieved the highest F1 score of [SCORE] on the test set.\n",
    "   - This indicates that [INSIGHT ABOUT THE MODEL/FEATURE COMBINATION].\n",
    "\n",
    "2. **Feature Representation Impact**:\n",
    "   - Traditional features (BoW, TF-IDF) performed [COMPARISON] compared to modern embeddings (Word2Vec, GloVe, BERT).\n",
    "   - This suggests that [INSIGHT ABOUT FEATURE REPRESENTATIONS FOR THIS TASK].\n",
    "\n",
    "3. **Classification Challenges**:\n",
    "   - The most commonly confused classes were [CLASS] misclassified as [CLASS].\n",
    "   - This is likely due to [REASON, e.g., semantic similarity, ambiguity in tweets, etc.].\n",
    "\n",
    "4. **Feature Importance**:\n",
    "   - SHAP analysis revealed that [KEY FEATURES] were most important for sentiment classification.\n",
    "   - These features correspond to [SEMANTIC MEANING, e.g., positive/negative sentiment words, specific topics].\n",
    "\n",
    "5. **Recommendations**:\n",
    "   - For practical applications, we recommend using [MODEL] with [FEATURE] due to [REASON].\n",
    "   - To further improve performance, we could [SUGGESTION, e.g., collect more data, use ensemble methods, etc.].\n",
    "\n",
    "These findings demonstrate the effectiveness of various approaches for Twitter sentiment analysis in gaming contexts and provide insights into how different feature representations capture sentiment information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}