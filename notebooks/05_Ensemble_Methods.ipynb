{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis - Ensemble Methods\n",
    "\n",
    "This optional notebook demonstrates how to combine multiple models using ensemble techniques:\n",
    "\n",
    "1. Model Stacking\n",
    "2. Weighted Averaging (Blending)\n",
    "3. Voting Ensemble\n",
    "\n",
    "We'll use the best models from the previous notebooks to create more robust predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style='whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# For Google Colab, uncomment these lines to mount Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# features_dir = '/content/drive/MyDrive/path/to/features'\n",
    "# models_dir = '/content/drive/MyDrive/path/to/models'\n",
    "# results_dir = '/content/drive/MyDrive/path/to/results'\n",
    "\n",
    "# For local development\n",
    "features_dir = '../data/features'\n",
    "models_dir = '../models'\n",
    "results_dir = '../results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Load model comparison results\n",
    "model_results = pd.read_csv(os.path.join(results_dir, 'model_comparison_results.csv'))\n",
    "\n",
    "# Get top performing models\n",
    "top_models = model_results.head(5)  # Get top 5 models\n",
    "print(\"Top 5 performing models:\")\n",
    "top_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Load test data\n",
    "# Load labels\n",
    "y = np.load(os.path.join(features_dir, 'labels.npy'))\n",
    "\n",
    "# Load features\n",
    "X_bow = sp.load_npz(os.path.join(features_dir, 'bow_features.npz'))\n",
    "X_tfidf = sp.load_npz(os.path.join(features_dir, 'tfidf_features.npz'))\n",
    "X_word2vec = np.load(os.path.join(features_dir, 'word2vec_features.npy'))\n",
    "X_glove = np.load(os.path.join(features_dir, 'glove_features.npy'))\n",
    "X_bert = np.load(os.path.join(features_dir, 'bert_features.npy'))\n",
    "\n",
    "print(\"Features loaded successfully.\")\n",
    "\n",
    "# Create train/val/test splits\n",
    "def split_data(X, y, test_size=0.15, val_size=0.15, random_state=42):\n",
    "    # First split: training + validation vs test\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Second split: training vs validation\n",
    "    # Adjust validation size to be a percentage of the training + validation set\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=val_ratio, random_state=random_state, stratify=y_train_val\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Get test sets\n",
    "_, _, bow_test, _, _, y_test = split_data(X_bow, y)\n",
    "_, _, tfidf_test, _, _, _ = split_data(X_tfidf, y, random_state=42)\n",
    "_, _, w2v_test, _, _, _ = split_data(X_word2vec, y, random_state=42)\n",
    "_, _, glove_test, _, _, _ = split_data(X_glove, y, random_state=42)\n",
    "_, _, bert_test, _, _, _ = split_data(X_bert, y, random_state=42)\n",
    "\n",
    "print(f\"Test set: {y_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Function to load a saved sklearn model\n",
    "def load_sklearn_model(model_path):\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "# Load label encoder\n",
    "with open(os.path.join(models_dir, 'label_encoder.pkl'), 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Display label encoding\n",
    "print(\"Label Encoding:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{i} -> {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get Model Predictions for Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Store base model predictions\n",
    "base_predictions = {}\n",
    "base_probabilities = {}\n",
    "\n",
    "# Load models and get predictions\n",
    "for _, row in top_models.iterrows():\n",
    "    model_name = row['Model']\n",
    "    feature_name = row['Feature']\n",
    "    model_key = f\"{model_name}_{feature_name}\"\n",
    "    \n",
    "    print(f\"Loading {model_name} with {feature_name}...\")\n",
    "    \n",
    "    # Get appropriate test features\n",
    "    if feature_name == 'BoW':\n",
    "        X_test_features = bow_test\n",
    "    elif feature_name == 'TF-IDF':\n",
    "        X_test_features = tfidf_test\n",
    "    elif feature_name == 'Word2Vec':\n",
    "        X_test_features = w2v_test\n",
    "    elif feature_name == 'GloVe':\n",
    "        X_test_features = glove_test\n",
    "    elif feature_name == 'BERT':\n",
    "        X_test_features = bert_test\n",
    "    \n",
    "    # Load model (skip BiLSTM for simplicity)\n",
    "    if model_name == 'BiLSTM':\n",
    "        print(\"Skipping BiLSTM for this ensemble example...\")\n",
    "        continue\n",
    "        \n",
    "    # Load sklearn model\n",
    "    model_path = os.path.join(models_dir, f'{model_name.lower()}_{feature_name.lower()}.pkl')\n",
    "    try:\n",
    "        model = load_sklearn_model(model_path)\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred = model.predict(X_test_features)\n",
    "        base_predictions[model_key] = y_pred\n",
    "        \n",
    "        # Get probabilities if available\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_proba = model.predict_proba(X_test_features)\n",
    "            base_probabilities[model_key] = y_proba\n",
    "        \n",
    "        # Check accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        print(f\"  Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  Model file not found: {model_path}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Simple Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Implement majority voting\n",
    "def majority_vote(predictions_dict):\n",
    "    # Stack all predictions\n",
    "    all_preds = np.column_stack([predictions_dict[model_key] for model_key in predictions_dict])\n",
    "    \n",
    "    # Get majority vote\n",
    "    majority_preds = np.zeros(all_preds.shape[0], dtype=int)\n",
    "    for i in range(all_preds.shape[0]):\n",
    "        unique, counts = np.unique(all_preds[i], return_counts=True)\n",
    "        majority_preds[i] = unique[np.argmax(counts)]\n",
    "    \n",
    "    return majority_preds\n",
    "\n",
    "# Apply majority voting\n",
    "majority_preds = majority_vote(base_predictions)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, majority_preds)\n",
    "f1 = f1_score(y_test, majority_preds, average='weighted')\n",
    "print(f\"Majority Voting Ensemble - Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, majority_preds, target_names=label_encoder.classes_))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "conf_matrix = confusion_matrix(y_test, majority_preds)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'Confusion Matrix - Majority Voting Ensemble')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'confusion_matrix_majority_voting.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Weighted Voting (Blending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Weighted voting based on validation performance\n",
    "def weighted_vote(probabilities_dict, weights_dict):\n",
    "    # Initialize weighted probabilities\n",
    "    weighted_proba = np.zeros((next(iter(probabilities_dict.values())).shape[0], \n",
    "                                next(iter(probabilities_dict.values())).shape[1]))\n",
    "    \n",
    "    # Get weighted sum of probabilities\n",
    "    total_weight = 0\n",
    "    for model_key, weight in weights_dict.items():\n",
    "        if model_key in probabilities_dict:\n",
    "            weighted_proba += probabilities_dict[model_key] * weight\n",
    "            total_weight += weight\n",
    "    \n",
    "    # Normalize by total weight\n",
    "    if total_weight > 0:\n",
    "        weighted_proba /= total_weight\n",
    "    \n",
    "    # Get class with highest probability\n",
    "    return np.argmax(weighted_proba, axis=1)\n",
    "\n",
    "# Define weights based on validation performance\n",
    "# For simplicity, we'll use the F1 scores from model_results\n",
    "weights = {}\n",
    "for _, row in top_models.iterrows():\n",
    "    model_name = row['Model']\n",
    "    feature_name = row['Feature']\n",
    "    model_key = f\"{model_name}_{feature_name}\"\n",
    "    \n",
    "    # Skip BiLSTM for this example\n",
    "    if model_name == 'BiLSTM':\n",
    "        continue\n",
    "        \n",
    "    # Use F1 score as weight\n",
    "    weights[model_key] = row['F1 Score']\n",
    "\n",
    "print(\"Model weights based on validation F1 scores:\")\n",
    "for model_key, weight in weights.items():\n",
    "    print(f\"{model_key}: {weight:.4f}\")\n",
    "\n",
    "# Apply weighted voting\n",
    "weighted_preds = weighted_vote(base_probabilities, weights)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, weighted_preds)\n",
    "f1 = f1_score(y_test, weighted_preds, average='weighted')\n",
    "print(f\"\\nWeighted Voting Ensemble - Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, weighted_preds, target_names=label_encoder.classes_))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "conf_matrix = confusion_matrix(y_test, weighted_preds)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'Confusion Matrix - Weighted Voting Ensemble')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'confusion_matrix_weighted_voting.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Model Stacking (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# For a complete stacking implementation, you would train base models on a training set,\n",
    "# generate predictions on a validation set, then train a meta-classifier on those predictions.\n",
    "# Here we'll use a simplified approach for demonstration purposes.\n",
    "\n",
    "# Create a dataframe with base model predictions\n",
    "base_preds_df = pd.DataFrame()\n",
    "for model_key, preds in base_predictions.items():\n",
    "    base_preds_df[model_key] = preds\n",
    "\n",
    "# Split into train/test for meta-classifier\n",
    "X_meta_train, X_meta_test, y_meta_train, y_meta_test = train_test_split(\n",
    "    base_preds_df, y_test, test_size=0.3, random_state=42, stratify=y_test\n",
    ")\n",
    "\n",
    "# Train meta-classifier\n",
    "meta_clf = LogisticRegression(max_iter=1000)\n",
    "meta_clf.fit(X_meta_train, y_meta_train)\n",
    "\n",
    "# Make predictions\n",
    "stacking_preds = meta_clf.predict(X_meta_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_meta_test, stacking_preds)\n",
    "f1 = f1_score(y_meta_test, stacking_preds, average='weighted')\n",
    "print(f\"Stacking Ensemble - Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_meta_test, stacking_preds, target_names=label_encoder.classes_))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "conf_matrix = confusion_matrix(y_meta_test, stacking_preds)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'Confusion Matrix - Stacking Ensemble')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'confusion_matrix_stacking.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Ensemble Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Create a comparison table\n",
    "# First, extract base model performance\n",
    "base_results = []\n",
    "for model_key, preds in base_predictions.items():\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds, average='weighted')\n",
    "    base_results.append({\n",
    "        'Model': model_key,\n",
    "        'Type': 'Base',\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1\n",
    "    })\n",
    "\n",
    "# Then add ensemble results\n",
    "ensemble_results = [\n",
    "    {\n",
    "        'Model': 'Majority Voting',\n",
    "        'Type': 'Ensemble',\n",
    "        'Accuracy': accuracy_score(y_test, majority_preds),\n",
    "        'F1 Score': f1_score(y_test, majority_preds, average='weighted')\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Weighted Voting',\n",
    "        'Type': 'Ensemble',\n",
    "        'Accuracy': accuracy_score(y_test, weighted_preds),\n",
    "        'F1 Score': f1_score(y_test, weighted_preds, average='weighted')\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Stacking',\n",
    "        'Type': 'Ensemble',\n",
    "        'Accuracy': accuracy_score(y_meta_test, stacking_preds),\n",
    "        'F1 Score': f1_score(y_meta_test, stacking_preds, average='weighted')\n",
    "    }\n",
    "]\n",
    "\n",
    "# Combine results\n",
    "all_results = pd.DataFrame(base_results + ensemble_results)\n",
    "\n",
    "# Sort by F1 score\n",
    "all_results = all_results.sort_values('F1 Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display results\n",
    "print(\"Model Performance Comparison:\")\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Visualize results\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Model', y='F1 Score', hue='Type', data=all_results)\n",
    "plt.title('Model Performance Comparison', fontsize=15)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('F1 Score', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'ensemble_comparison.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Best Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "source": [
    "# Identify best ensemble method\n",
    "best_ensemble = all_results[all_results['Type'] == 'Ensemble'].iloc[0]['Model']\n",
    "print(f\"Best ensemble method: {best_ensemble}\")\n",
    "\n",
    "# For weighted voting, save the weights\n",
    "if best_ensemble == 'Weighted Voting':\n",
    "    with open(os.path.join(models_dir, 'weighted_ensemble_weights.pkl'), 'wb') as f:\n",
    "        pickle.dump(weights, f)\n",
    "    print(f\"Weighted ensemble weights saved to {os.path.join(models_dir, 'weighted_ensemble_weights.pkl')}\")\n",
    "\n",
    "# For stacking, save the meta-classifier\n",
    "if best_ensemble == 'Stacking':\n",
    "    with open(os.path.join(models_dir, 'stacking_meta_classifier.pkl'), 'wb') as f:\n",
    "        pickle.dump(meta_clf, f)\n",
    "    print(f\"Stacking meta-classifier saved to {os.path.join(models_dir, 'stacking_meta_classifier.pkl')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explored three ensemble methods for combining multiple sentiment analysis models:\n",
    "\n",
    "1. **Majority Voting**: A simple approach that takes the most common prediction across base models.\n",
    "2. **Weighted Voting (Blending)**: A more sophisticated approach that weights each model's prediction by its validation performance.\n",
    "3. **Stacking**: A meta-learning approach that trains a higher-level model on the predictions of base models.\n",
    "\n",
    "Key findings:\n",
    "\n",
    "- The best single model was [FILL IN BASED ON RESULTS].\n",
    "- The best ensemble method was [FILL IN BASED ON RESULTS].\n",
    "- Ensemble methods [DID/DID NOT] improve over the best individual model by [PERCENTAGE].\n",
    "\n",
    "The benefits of ensemble methods include:\n",
    "- Reduced variance and increased robustness\n",
    "- Better generalization across different types of data\n",
    "- Potential for higher performance by combining complementary models\n",
    "\n",
    "For production deployment, we recommend using [FILL IN BASED ON RESULTS] due to its balance of performance and complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}